{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create 1 dataframe from all four modules\n",
    "Shuffle and Randomly partition that one dataframe \n",
    "\n",
    "80%train + Cross validation: This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.\n",
    "\n",
    "##############\n",
    "20%test: Once we have chosen our preferred prediction algorithm but we don't know yet how it's going to perform on \n",
    "completely new real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how \n",
    "it's going to perform so we can have an idea about our algorithm's performance on new data.\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateDF(featuresFile, profiledFile):\n",
    "    #reads featurefiles and projects 3 columns (ppn, sizeGB, Ytime)\n",
    "    #reads profiledFile (the table from profiling) and reads all 55 columns\n",
    "    #generates one dataframe with 58 columns and n rows (=size of profiled data from csv)\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    header='ft1 ft2 ft3 ft4 ft5 ft6 ft7 ft8 ft9 ft10 ft11 ft12 ft13 ft14 ft15 ft16 ft17 \\\n",
    "    ft18 ft24 ft25 ft19 ft39 ft20 ft33 ft21 ft35 ft22 ft23 ft34 ft36 ft37 ft38 ft40 ft41 \\\n",
    "    ft42 ft43 ft44 ft45 ft46 ft48 ft47 ft49 ft51 ft50 ft52 ft53 ft54 ft55 ft26 ft27 ft28 ft29 ft30 ft31 ft32'\n",
    "    \n",
    "    def value(item):\n",
    "        return item[item.find('=')+1:]\n",
    "    \n",
    "    print(\"Reading features from %s\" % featuresFile)\n",
    "    df_features = pd.read_table(featuresFile, header=None, delimiter=',',\n",
    "                       converters={i:value for i in range(55)},\n",
    "                       names=header.split())\n",
    "    df_features = df_features.astype(float)\n",
    "    #print(df_features.head(2))\n",
    "    \n",
    "    ################################\n",
    "    #read profiledFile\n",
    "    df_profiled = pd.read_csv(profiledFile)\n",
    "    if 'Unnamed: 0' in df_profiled.columns:\n",
    "        del df_profiled['Unnamed: 0']\n",
    "    df_profiled = df_profiled.astype(float)\n",
    "    rows = df_profiled.shape[0]\n",
    "    \n",
    "    #project columns of use\n",
    "    df_profiled = df_profiled[['ppn','sizeGB','Y_time']]\n",
    "    print(\"Reading profiled file %s\\t: %s\" % (profiledFile,str(df_profiled.shape)))\n",
    "    \n",
    "    ################################\n",
    "    # create dataframe from featuresFile static program features with same #ROWs\n",
    "    frames = [df_features for i in range(rows)]\n",
    "    program_ft = pd.concat(frames)\n",
    "    program_ft.reset_index(inplace=True)\n",
    "    del program_ft['index']\n",
    "    print(\"Shape of program static features\\t\\t\\t\\t\\t: %s \" % str(program_ft.shape))\n",
    "    \n",
    "    ################################\n",
    "    # concatenate static features (55) with profiled data (3)\n",
    "    ft_plus_profiled = pd.concat([df_profiled, program_ft], axis=1, join_axes=[df_profiled.index])\n",
    "    ft_plus_profiled['y_time']=ft_plus_profiled['Y_time'] # so that y_time is last column\n",
    "    del ft_plus_profiled['Y_time']\n",
    "    print(\"Returning concatenated data frame (ft + profiled data)\\t\\t\\t: %s\" % str(ft_plus_profiled.shape))\n",
    "    #display(ft_plus_profiled.tail(5))    \n",
    "    \n",
    "    return ft_plus_profiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack two dataframes on top of each other\n",
    "def stackDFs(df1, df2):\n",
    "    import copy\n",
    "    import pandas as pd\n",
    "    \n",
    "    frames = [copy.deepcopy(df1), copy.deepcopy(df2)]\n",
    "    both = pd.concat(frames)\n",
    "    both.reset_index(inplace=True)\n",
    "    del both['index']\n",
    "    return both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "################################## cdhitdup ################################################\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from ./features/cdhitdup_singlecmd_features.txt\n",
      "Reading profiled file ./profiledcsvfiles/cdhitdup_ppn_sizeGB_Ytime.csv\t: (32, 3)\n",
      "Shape of program static features\t\t\t\t\t: (32, 55) \n",
      "Returning concatenated data frame (ft + profiled data)\t\t\t: (32, 58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 58)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cdhitdup = generateDF('./features/cdhitdup_singlecmd_features.txt','./profiledcsvfiles/cdhitdup_ppn_sizeGB_Ytime.csv')\n",
    "df_cdhitdup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "##################################### frhit ################################################\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from ./features/frhit_singlecmd_features.txt\n",
      "Reading profiled file ./profiledcsvfiles/frhit_ppn_sizeGB_Ytime.csv\t: (96, 3)\n",
      "Shape of program static features\t\t\t\t\t: (96, 55) \n",
      "Returning concatenated data frame (ft + profiled data)\t\t\t: (96, 58)\n"
     ]
    }
   ],
   "source": [
    "df_frhit = generateDF('./features/frhit_singlecmd_features.txt', './profiledcsvfiles/frhit_ppn_sizeGB_Ytime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "##################################### velvetH ##############################################\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from ./features/velveth_singlecmd_features.txt\n",
      "Reading profiled file ./profiledcsvfiles/velvetH_ppn_sizeGB_Ytime.csv\t: (97, 3)\n",
      "Shape of program static features\t\t\t\t\t: (97, 55) \n",
      "Returning concatenated data frame (ft + profiled data)\t\t\t: (97, 58)\n"
     ]
    }
   ],
   "source": [
    "df_hvelvetH = generateDF('./features/velveth_singlecmd_features.txt','./profiledcsvfiles/velvetH_ppn_sizeGB_Ytime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "##################################### velvetG ##############################################\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from ./features/velvetg_singlecmd_features.txt\n",
      "Reading profiled file ./profiledcsvfiles/velvetG_ppn_sizeGB_Ytime.csv\t: (97, 3)\n",
      "Shape of program static features\t\t\t\t\t: (97, 55) \n",
      "Returning concatenated data frame (ft + profiled data)\t\t\t: (97, 58)\n"
     ]
    }
   ],
   "source": [
    "df_gvelvetG = generateDF('./features/velvetg_singlecmd_features.txt','./profiledcsvfiles/velvetG_ppn_sizeGB_Ytime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A=cdhitdup, B=frhit, C=velvetG, D=velvetH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a fraction from each dataframe: A,B,C,D\n",
    "fractn = 0.8\n",
    "\n",
    "####### Start of picking a fraction\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  for A - cdhitdup\n",
    "test_A, train_A = train_test_split(df_cdhitdup.dropna(), test_size = fractn)\n",
    "\n",
    "#  for B - frhit\n",
    "test_B, train_B = train_test_split(df_frhit.dropna(), test_size = fractn)\n",
    "\n",
    "#  for C - velvetg\n",
    "test_C, train_C = train_test_split(df_gvelvetG.dropna(), test_size = fractn)\n",
    "\n",
    "#  for D - velveth\n",
    "test_D, train_D = train_test_split(df_hvelvetH.dropna(), test_size = fractn)\n",
    "\n",
    "####### End of picking a fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 58) (26, 58)\n",
      "(19, 58) (76, 58)\n",
      "(19, 58) (78, 58)\n",
      "(19, 58) (78, 58)\n"
     ]
    }
   ],
   "source": [
    "print(test_A.shape, train_A.shape)\n",
    "print(test_B.shape, train_B.shape)\n",
    "print(test_C.shape, train_C.shape)\n",
    "print(test_D.shape, train_D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Index(['ppn', 'sizeGB', 'ft1', 'ft2', 'ft3', 'ft4', 'ft5', 'ft6', 'ft7', 'ft8',\n",
      "       'ft9', 'ft10', 'ft11', 'ft12', 'ft13', 'ft14', 'ft15', 'ft16', 'ft17',\n",
      "       'ft18', 'ft24', 'ft25', 'ft19', 'ft39', 'ft20', 'ft33', 'ft21', 'ft35',\n",
      "       'ft22', 'ft23', 'ft34', 'ft36', 'ft37', 'ft38', 'ft40', 'ft41', 'ft42',\n",
      "       'ft43', 'ft44', 'ft45', 'ft46', 'ft48', 'ft47', 'ft49', 'ft51', 'ft50',\n",
      "       'ft52', 'ft53', 'ft54', 'ft55', 'ft26', 'ft27', 'ft28', 'ft29', 'ft30',\n",
      "       'ft31', 'ft32'],\n",
      "      dtype='object')\n",
      "1 ['y_time']\n"
     ]
    }
   ],
   "source": [
    "# Stack Train\n",
    "\n",
    "frames = [train_A, train_B, train_C, train_D]\n",
    "result = pd.concat(frames)\n",
    "result.reset_index(inplace=True)\n",
    "del result['index']\n",
    "\n",
    "features = result.columns[0:57]\n",
    "target   = [result.columns[57]]\n",
    "print(len(features), features)\n",
    "print(len(target), target)\n",
    "\n",
    "trainDataFrame = result.copy(deep=True)\n",
    "\n",
    "# Separate X, y\n",
    "X_train = trainDataFrame[features]\n",
    "y_train = trainDataFrame[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Index(['ppn', 'sizeGB', 'ft1', 'ft2', 'ft3', 'ft4', 'ft5', 'ft6', 'ft7', 'ft8',\n",
      "       'ft9', 'ft10', 'ft11', 'ft12', 'ft13', 'ft14', 'ft15', 'ft16', 'ft17',\n",
      "       'ft18', 'ft24', 'ft25', 'ft19', 'ft39', 'ft20', 'ft33', 'ft21', 'ft35',\n",
      "       'ft22', 'ft23', 'ft34', 'ft36', 'ft37', 'ft38', 'ft40', 'ft41', 'ft42',\n",
      "       'ft43', 'ft44', 'ft45', 'ft46', 'ft48', 'ft47', 'ft49', 'ft51', 'ft50',\n",
      "       'ft52', 'ft53', 'ft54', 'ft55', 'ft26', 'ft27', 'ft28', 'ft29', 'ft30',\n",
      "       'ft31', 'ft32'],\n",
      "      dtype='object')\n",
      "1 ['y_time']\n"
     ]
    }
   ],
   "source": [
    "# Stack Test\n",
    "\n",
    "frames = [test_A, test_B, test_C, test_D]\n",
    "result = pd.concat(frames)\n",
    "result.reset_index(inplace=True)\n",
    "del result['index']\n",
    "\n",
    "features = result.columns[0:57]\n",
    "target   = [result.columns[57]]\n",
    "print(len(features), features)\n",
    "print(len(target), target)\n",
    "\n",
    "testDataFrame = result.copy(deep=True)\n",
    "\n",
    "# Separate X, y\n",
    "X_test = testDataFrame[features]\n",
    "y_test = testDataFrame[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate X, y\n",
    "X_test_A = test_A[features]\n",
    "y_test_A = test_A[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate X, y\n",
    "X_test_B = test_B[features]\n",
    "y_test_B = test_B[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate X, y\n",
    "X_test_C = test_C[features]\n",
    "y_test_C = test_C[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate X, y\n",
    "X_test_D = test_D[features]\n",
    "y_test_D = test_D[target].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((258, 57), (63, 57))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = { 'X_train': X_train, 'y_train': y_train, 'X_test':X_test, 'y_test': y_test }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add to dictionary: { X_test_A, y_test_A }\n",
    "data['X_test_A']=X_test_A\n",
    "data['y_test_A']=y_test_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add to dictionary: { X_test_B, y_test_B }\n",
    "data['X_test_B']=X_test_B\n",
    "data['y_test_B']=y_test_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add to dictionary: { X_test_C, y_test_C }\n",
    "data['X_test_C']=X_test_C\n",
    "data['y_test_C']=y_test_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_test_A', 'y_test_A', 'X_test_B', 'y_test_B', 'X_test_C', 'y_test_C', 'X_test_D', 'y_test_D'])\n"
     ]
    }
   ],
   "source": [
    "# Add to dictionary: { X_test_D, y_test_D }\n",
    "data['X_test_D']=X_test_D\n",
    "data['y_test_D']=y_test_D\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train 258\n",
      "y_train 258\n",
      "X_test 63\n",
      "y_test 63\n",
      "X_test_A 6\n",
      "y_test_A 6\n",
      "X_test_B 19\n",
      "y_test_B 19\n",
      "X_test_C 19\n",
      "y_test_C 19\n",
      "X_test_D 19\n",
      "y_test_D 19\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key, len(data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WRITE the dictionary 'd' back to file 'datasetIPPD.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write to file\n",
    "import pickle\n",
    "\n",
    "with open('datasetIPPD.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Partitioning Date: 2017-04-12 15:08\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"Data Partitioning Date: %s\" % datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
