{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1   LassoCVModel (filename):\n",
    "\n",
    "2   OMPCVModel (filename):\n",
    "\n",
    "3   RidgeCVModel (filename):\n",
    "\n",
    "4   ElasticNetCVModel (filename):\n",
    "\n",
    "5   GradientBoostingCVModel (filename):\n",
    "\n",
    "6   RandomForestCVModel (filename):\n",
    "\n",
    "7   SVRSigmoidCVModel (filename):\n",
    "\n",
    "8   SVRRbfCVModel (filename):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "import numpy as np\n",
    "c = 0\n",
    "\n",
    "def rmse_scorer(model, X, y):\n",
    "    global c\n",
    "    y_predict = model.predict(X)\n",
    "    k = np.sqrt(mean_squared_error(y, y_predict))\n",
    "    c = c+1\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>>\n",
    "<<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>>\n",
    "<<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>><<>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "def RandomForestCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.ensemble.forest import RandomForestRegressor\n",
    "    from numpy.random import RandomState\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "    tuned_parameters.append( { \n",
    "                              \"n_estimators\" :[2000, 4000, 6000, 10000]\n",
    "                            })\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "\n",
    "    grdsurch = GridSearchCV(RandomForestRegressor(n_estimators=6000, criterion='mse', \n",
    "                                                  max_depth=None, \n",
    "                                                  min_samples_split=2, \n",
    "                                                  min_samples_leaf=1, \n",
    "                                                  min_weight_fraction_leaf=0.0, \n",
    "                                                  max_features='auto', \n",
    "                                                  max_leaf_nodes=None, \n",
    "                                                  min_impurity_split=1e-07, \n",
    "                                                  bootstrap=True, oob_score=True, \n",
    "                                                  n_jobs=-1, random_state=None, \n",
    "                                                  verbose=0, warm_start=False), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 OK\n",
    "def GradientBoostingCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from numpy.random import RandomState\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "\n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    ##############################################################\n",
    "    tuned_parameters = [     {  \"n_estimators\" :[2000, 4000, 6000, 10000],\n",
    "                                \"loss\" : ['ls'],\n",
    "                                \"learning_rate\": [0.001, 0.005, 0.01]\n",
    "                             }\n",
    "                       ]\n",
    "\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "\n",
    "    #Boosting\n",
    "    grdsurch = GridSearchCV(GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=6000, subsample=1.0, \n",
    "                                                      criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "                                                      min_weight_fraction_leaf=0.0, max_depth=None, \n",
    "                                                      min_impurity_split=1e-07, \n",
    "                                                      init=None, random_state=None, max_features=None, alpha=0.9, \n",
    "                                                      verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto'), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "\n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_c():\n",
    "    # c counts number of times scorer is called\n",
    "    global c\n",
    "    c=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "def RidgeCVModel(filename, scale=True):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV \n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    \n",
    "    ############################################################## RidgeL2\n",
    "    tuned_parameters = []\n",
    "    tuned_parameters.append( {'alpha' : np.logspace(-35, +25, 100) } ) \n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print(tuned_parameters)\n",
    "    print(\"##########################\")\n",
    "\n",
    "    # Ridge Regression (L2)\n",
    "    grdsurch = GridSearchCV(Ridge(alpha=1.0, fit_intercept=True, \n",
    "                             normalize=False, copy_X=True, max_iter=None, tol=1e-6, \n",
    "                             solver='auto', random_state=None), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    print(\"\\n\\n scorer is called: %d times \\n\\n\" % c)\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was RidgeCVModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6\n",
    "def ElasticNetCVModel(filename, scale=True):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    ############################################################## ElasticNet\n",
    "    tuned_parameters = []\n",
    "    tuned_parameters.append( { \n",
    "                               'alpha'   : np.logspace(-25, +25, 40), \n",
    "                               'l1_ratio': [0.9, 0.75, 0.5, .25, .1]  \n",
    "                             }\n",
    "                           ) \n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print(tuned_parameters)\n",
    "    print(\"##########################\")\n",
    "\n",
    "    # Elastic Net (L1 + L2)\n",
    "    # Linear regression with combined L1 and L2 priors as regularizer\n",
    "    #\n",
    "    grdsurch = GridSearchCV(ElasticNet(alpha=3.7926901907322537e-14, copy_X=True, fit_intercept=True,\n",
    "      l1_ratio=0.9, max_iter=10000000.0, normalize=False, positive=False,\n",
    "      precompute=False, random_state=None, selection='cyclic', tol=1e-20,\n",
    "      warm_start=False), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    \n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#7\n",
    "def SVRPolyCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "        \n",
    "    # -- a -- ‘poly’ \n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "    \n",
    "    tuned_parameters.append({\n",
    "                             'gamma'  : np.logspace(-15, 3, 2),\n",
    "                             'C'      : np.logspace(-5, 15, 2),\n",
    "                             'degree' : [2]\n",
    "                            })\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters... \")\n",
    "    print()\n",
    "\n",
    "    grdsurch = GridSearchCV(SVR(kernel='poly', degree=3, \n",
    "                                gamma='auto', coef0=0.0, tol=1e-2, C=1.0, \n",
    "                                epsilon=0.1, shrinking=False, cache_size=20*1024, \n",
    "                                verbose=False, max_iter=1e9), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was SVRPoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8\n",
    "def SVRSigmoidCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import numpy as np\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################   \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    # -- b -- ‘sigmoid’\n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "    \n",
    "    tuned_parameters.append({\n",
    "                             'gamma'  : np.logspace(-15, 3, 1),\n",
    "                             'C'      : np.logspace(-5, 15, 1)\n",
    "                            }\n",
    "                            )    \n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "\n",
    "    grdsurch = GridSearchCV(SVR(kernel='sigmoid', degree=3, \n",
    "                                gamma='auto', coef0=0.0, tol=1e-7, C=1.0, \n",
    "                                epsilon=0.1, shrinking=False, cache_size=20*1024, \n",
    "                                verbose=False, max_iter=-1), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was SVRSigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "def SVRLinearCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    # -- c -- ‘linear’ \n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "    \n",
    "    tuned_parameters.append({\n",
    "                             # 'gamma'  : np.logspace(-15, 3, 5),\n",
    "                             'C'      : np.logspace(-15, -5, 5)\n",
    "                            })\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "\n",
    "    grdsurch = GridSearchCV(SVR(kernel='linear', degree=3, \n",
    "                                gamma='auto', coef0=0.0, tol=1e-7, C=1.0, \n",
    "                                epsilon=0.1, shrinking=False, cache_size=20*1024, \n",
    "                                verbose=False, max_iter=-1), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\n Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was SVRLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "def SVRRbfCVModel(filename, scale=True):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################  \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    # -- d -- ‘rbf’\n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "\n",
    "    tuned_parameters.append({ \n",
    "                             'gamma'  : np.logspace(-15, 3, 5),\n",
    "                             'C'      : np.logspace(-5, 15, 5)\n",
    "                             })\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "\n",
    "    grdsurch = GridSearchCV(SVR(kernel='rbf', degree=3, \n",
    "                                gamma='auto', coef0=0.0, tol=1e-7, C=1.0, \n",
    "                                epsilon=0.1, shrinking=False, cache_size=100*1024, \n",
    "                                verbose=False, max_iter=-1), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was SVRrbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## OK below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_this(scale, X_trai0, X_tes0):\n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "    import copy\n",
    "    \n",
    "    X_trai = X_trai0.copy()\n",
    "    X_tes  = X_tes0.copy()\n",
    "    \n",
    "    if(scale):\n",
    "        ##################################################\n",
    "        # Scale X\n",
    "        print('\\n{{ scale_this: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Scaling X_train and X_test... }}\\n')\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        X_train = standard_scaler.fit_transform(X_trai)\n",
    "        X_test  = standard_scaler.transform(X_tes)\n",
    "        ##################################################\n",
    "    else:\n",
    "        X_train = X_trai\n",
    "        X_test  = X_tes\n",
    "        ##################################################\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.logspace(2, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 OK\n",
    "def LassoCVModel(filename, scale=True):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################   \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "    \n",
    "    \n",
    "    ############################################################## Lasso\n",
    "    tuned_parameters = []\n",
    "    tuned_parameters.append( {'alpha' : np.logspace(-4, 10, 50),\n",
    "                              'precompute' : [True, False]\n",
    "                             } )\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print(tuned_parameters)\n",
    "    print(\"##########################\")\n",
    "\n",
    "    # Lasso (L1)\n",
    "    grdsurch = GridSearchCV(Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, \n",
    "                                  copy_X=True, max_iter=1e7, tol=1e-6, warm_start=False, \n",
    "                                  positive=False, random_state=None, selection='random'), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "    \n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was Lasso (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 OK\n",
    "def OMPCVModel(filename, scale=False):\n",
    "    #open file and get the dictionary\n",
    "    import pickle\n",
    "    from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    #extract X_train, y_train, X_test, t_test\n",
    "    X_trai  = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_tes   = data['X_test']\n",
    "    y_test  = data['y_test']\n",
    "        \n",
    "    ############ scaling of features #################\n",
    "    \n",
    "    X_train, X_test = scale_this(scale, X_trai, X_tes)\n",
    "    \n",
    "    ##################################################   \n",
    "    print(\"Dataset size read: train %d and test %d \\n\" %(len(y_train), len(y_test)))\n",
    "\n",
    "    #Normalize\n",
    "    #X_train = preprocessing.normalize(X_train, norm='l1')\n",
    "    #X_test  = preprocessing.normalize(X_test,  norm='l1')\n",
    "    \n",
    "    #\n",
    "    ##############################################################\n",
    "    tuned_parameters = []\n",
    "    tuned_parameters.append({'tol' : [1e-4],\n",
    "                             'n_nonzero_coefs' : [14, 28]\n",
    "                            \n",
    "                            })\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters \")\n",
    "    print()\n",
    "    \n",
    "    # OMP\n",
    "    grdsurch = GridSearchCV(OrthogonalMatchingPursuit(n_nonzero_coefs=3, \n",
    "                                                      tol=1e-15, fit_intercept=True, \n",
    "                                                      normalize=False, precompute='auto'), \n",
    "                       tuned_parameters, \n",
    "                       cv=3, \n",
    "                       n_jobs=-1, \n",
    "                       scoring=rmse_scorer)\n",
    "    \n",
    "    print('Starting grdsurch.fit(X_train, y_train)')\n",
    "    \n",
    "    grdsurch.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters set found on development set:\")\n",
    "    print()\n",
    "    print(grdsurch.best_params_)\n",
    "    \n",
    "    print(grdsurch.best_estimator_)\n",
    "    print()\n",
    "    rmse_cv = grdsurch.best_score_\n",
    "\n",
    "    #Reporting Score on Test Set\n",
    "    model               = grdsurch.best_estimator_\n",
    "    reporting_testscore = rmse_scorer(model, X_test, y_test)\n",
    "\n",
    "    ###########################\n",
    "    # added for measure predictions on X_test_A, X_test_B ...\n",
    "    print('Full Test Set: %d' % len(y_test))\n",
    "    display(data['y_test'])\n",
    "    display(model.predict(X_test))\n",
    "    \n",
    "    reporting_testscoreA = None\n",
    "    reporting_testscoreB = None\n",
    "    reporting_testscoreC = None\n",
    "    reporting_testscoreD = None\n",
    "    test_mean_y_comparingA = None\n",
    "    test_mean_y_comparingB = None\n",
    "    test_mean_y_comparingC = None\n",
    "    test_mean_y_comparingD = None        \n",
    "        \n",
    "    if('y_test_A' in data):\n",
    "        print('A: %d' % len(data['y_test_A']))\n",
    "        X_train, X_test_A = scale_this(scale, X_trai, data['X_test_A'])\n",
    "        \n",
    "        reporting_testscoreA = rmse_scorer(model, X_test_A, data['y_test_A'])\n",
    "        display(data['y_test_A'])\n",
    "        display(model.predict(X_test_A))\n",
    "        test_mean_y_comparingA = data['y_test_A'].mean()\n",
    "\n",
    "    if('y_test_B' in data):\n",
    "        print('B: %d' % len(data['y_test_B']))\n",
    "        X_train, X_test_B = scale_this(scale, X_trai, data['X_test_B'])\n",
    "        \n",
    "        reporting_testscoreB = rmse_scorer(model, X_test_B, data['y_test_B'])\n",
    "        display(data['y_test_B'])\n",
    "        display(model.predict(X_test_B))\n",
    "        test_mean_y_comparingB = data['y_test_B'].mean()\n",
    "\n",
    "    if('y_test_C' in data):\n",
    "        print('C: %d' % len(data['y_test_C']))\n",
    "        X_train, X_test_C = scale_this(scale, X_trai, data['X_test_C'])\n",
    "        \n",
    "        reporting_testscoreC = rmse_scorer(model, X_test_C, data['y_test_C'])\n",
    "        display(data['y_test_C'])\n",
    "        display(model.predict(X_test_C))\n",
    "        test_mean_y_comparingC = data['y_test_C'].mean()\n",
    "\n",
    "    if('y_test_D' in data):\n",
    "        print('D: %d' % len(data['y_test_D']))\n",
    "        X_train, X_test_D = scale_this(scale, X_trai, data['X_test_D'])\n",
    "        \n",
    "        reporting_testscoreD = rmse_scorer(model, X_test_D, data['y_test_D'])\n",
    "        display(data['y_test_D'])\n",
    "        display(model.predict(X_test_D))\n",
    "        test_mean_y_comparingD = data['y_test_D'].mean()\n",
    "    \n",
    "    return {filename: {'train_rmse_cv_picking': rmse_cv, \n",
    "                       'test_rmse_reporting' : reporting_testscore,\n",
    "                       'test_rmse_reportingA': reporting_testscoreA,\n",
    "                       'test_rmse_reportingB': reporting_testscoreB,\n",
    "                       'test_rmse_reportingC': reporting_testscoreC,\n",
    "                       'test_rmse_reportingD': reporting_testscoreD,\n",
    "                       'test_mean_y_comparing': y_test.mean(),\n",
    "                       'test_mean_y_comparingA': test_mean_y_comparingA,\n",
    "                       'test_mean_y_comparingB': test_mean_y_comparingB,\n",
    "                       'test_mean_y_comparingC': test_mean_y_comparingC,\n",
    "                       'test_mean_y_comparingD': test_mean_y_comparingD,\n",
    "                       'model': model\n",
    "                      }}\n",
    "\n",
    "#this was OMPCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
