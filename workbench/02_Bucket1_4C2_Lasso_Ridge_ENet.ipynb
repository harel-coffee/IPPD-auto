{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import all the filenames\n",
    "from allfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  def LassoCVModel(filename):\n",
    "\n",
    "3  def RidgeCVModel(filename):\n",
    "\n",
    "4  def ElasticNetCVModel(filename):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "    if __name__ == '__main__':\n",
    "        global model_selection\n",
    "        global functionList\n",
    "        global filenames\n",
    "        \n",
    "        functionList = [ LassoCVModel, GradientBoostingCVModel, \n",
    "                 RandomForestCVModel, RidgeCVModel, ElasticNetCVModel, \n",
    "                 SVRPolyCVModel, SVRSigmoidCVModel, SVRLinearCVModel, \n",
    "                 SVRRbfCVModel ] #OMPCVModel\n",
    "\n",
    "        filenames = ['AB.pickle', 'BC.pickle', 'CD.pickle', 'AC.pickle', 'AD.pickle', 'BD.pickle' ]\n",
    "        \n",
    "        model_selection = {}\n",
    "        \n",
    "        for f in functionList:\n",
    "            print()\n",
    "            for filename in filenames:\n",
    "                out = f(filename)\n",
    "                newScore = out[filename]['train_rmse_cv_4_picking']\n",
    "\n",
    "                if filename in model_selection:\n",
    "                    #compare scores and select more accurate model\n",
    "                    oldScore = model_selection[filename]['train_rmse_cv_4_picking']\n",
    "                    if(newScore < oldScore):\n",
    "                        print(\"【【【】】】switching models for %s...\" % filename)\n",
    "                        print(\"【【【】】】newScore: %f < oldScore: %f\" % (newScore, oldScore))\n",
    "                        print(\"【【【】】】newModel: %s \\n\" % f)\n",
    "                        print(100*'_')\n",
    "                        model_selection[filename] = out[filename]\n",
    "                else:\n",
    "                    #no score to compare\n",
    "                    print(\"【【】】adding new model for %s \\n\" % filename)\n",
    "                    print(100*'_')\n",
    "                    model_selection[filename] = out[filename]\n",
    "    return model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_lasso = wrapper([LassoCVModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_print = t_lasso\n",
    "\n",
    "for key in t_print.keys():\n",
    "    print(80*'_')\n",
    "    \n",
    "    lasso_model = t_print[key]['model']\n",
    "    print('Model: %s' % (str(t_print[key]['model'])[:20]))\n",
    "    print('Train RMSE: %f \\n' % (t_print[key]['train_rmse_cv_picking']))\n",
    "    \n",
    "    #print('Test RMSE   : %f' % (t_print[key]['test_rmse_reporting']))\n",
    "    #print('Overall Test Mean: %f \\n' % (t_print[key]['test_mean_y_comparing']))\n",
    "    \n",
    "    print('A...TestA RMSE   : %f' % (t_print[key]['test_rmse_reportingA']))\n",
    "    print('A...<<y_test_A Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingA']))\n",
    "    \n",
    "    print('B...TestB RMSE   : %f' % (t_print[key]['test_rmse_reportingB']))\n",
    "    print('B...<<y_test_B Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingB']))\n",
    "    \n",
    "    print('C...TestC RMSE   : %f' % (t_print[key]['test_rmse_reportingC']))\n",
    "    print('C...<<y_test_C Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingC']))\n",
    "    \n",
    "    print('D...TestD RMSE   : %f' % (t_print[key]['test_rmse_reportingD']))\n",
    "    print('D...<<y_test_D Mean>>: %f' % (t_print[key]['test_mean_y_comparingD']))\n",
    "    \n",
    "    print(80*'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_ridge = wrapper([RidgeCVModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_print = t_ridge\n",
    "\n",
    "for key in t_print.keys():\n",
    "    print(80*'_')\n",
    "    \n",
    "    lasso_model = t_print[key]['model']\n",
    "    print('Model: %s' % (str(t_print[key]['model'])[:200]))\n",
    "    print('Train RMSE: %f \\n' % (t_print[key]['train_rmse_cv_picking']))\n",
    "    \n",
    "    #print('Test RMSE   : %f' % (t_print[key]['test_rmse_reporting']))\n",
    "    #print('Overall Test Mean: %f \\n' % (t_print[key]['test_mean_y_comparing']))\n",
    "    \n",
    "    print('A...TestA RMSE   : %f' % (t_print[key]['test_rmse_reportingA']))\n",
    "    print('A...<<y_test_A Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingA']))\n",
    "    \n",
    "    print('B...TestB RMSE   : %f' % (t_print[key]['test_rmse_reportingB']))\n",
    "    print('B...<<y_test_B Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingB']))\n",
    "    \n",
    "    print('C...TestC RMSE   : %f' % (t_print[key]['test_rmse_reportingC']))\n",
    "    print('C...<<y_test_C Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingC']))\n",
    "    \n",
    "    print('D...TestD RMSE   : %f' % (t_print[key]['test_rmse_reportingD']))\n",
    "    print('D...<<y_test_D Mean>>: %f' % (t_print[key]['test_mean_y_comparingD']))\n",
    "    \n",
    "    print(80*'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>\n",
    "<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:><:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:<:>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_elasticNet = wrapper([ElasticNetCVModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_print = t_elasticNet\n",
    "\n",
    "for key in t_print.keys():\n",
    "    print(80*'_')\n",
    "    \n",
    "    lasso_model = t_print[key]['model']\n",
    "    print('Model: %s' % (str(t_print[key]['model'])[:200]))\n",
    "    print('Train RMSE: %f \\n' % (t_print[key]['train_rmse_cv_picking']))\n",
    "    \n",
    "    #print('Test RMSE   : %f' % (t_print[key]['test_rmse_reporting']))\n",
    "    #print('Overall Test Mean: %f \\n' % (t_print[key]['test_mean_y_comparing']))\n",
    "    \n",
    "    print('A...TestA RMSE   : %f' % (t_print[key]['test_rmse_reportingA']))\n",
    "    print('A...<<y_test_A Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingA']))\n",
    "    \n",
    "    print('B...TestB RMSE   : %f' % (t_print[key]['test_rmse_reportingB']))\n",
    "    print('B...<<y_test_B Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingB']))\n",
    "    \n",
    "    print('C...TestC RMSE   : %f' % (t_print[key]['test_rmse_reportingC']))\n",
    "    print('C...<<y_test_C Mean>>: %f \\n' % (t_print[key]['test_mean_y_comparingC']))\n",
    "    \n",
    "    print('D...TestD RMSE   : %f' % (t_print[key]['test_rmse_reportingD']))\n",
    "    print('D...<<y_test_D Mean>>: %f' % (t_print[key]['test_mean_y_comparingD']))\n",
    "    \n",
    "    print(80*'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
